{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38e177cf",
      "metadata": {
        "id": "38e177cf"
      },
      "source": [
        "# Deep N-grams (GRU LM & LSTM LM)\n",
        "\n",
        "In this Notebook you will explore Recurrent Neural Networks `RNN`.\n",
        "\n",
        "In this notebook we will apply the following steps:\n",
        "- Convert a line of text into a tensor\n",
        "- Create a tensorflow dataset\n",
        "- Define a GRU and LSTM model using `TensorFlow`\n",
        "- Train the models using `TensorFlow`\n",
        "- Compute the accuracy of our models using the perplexity\n",
        "- Generate text using our own models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3918e9de",
      "metadata": {
        "id": "3918e9de"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "In this Notebook, we'll delve into the world of text generation using Recurrent Neural Networks (RNNs). The primary objective is to predict the next set of characters based on the preceding ones. This seemingly straightforward task holds immense practicality in applications like predictive text and creative writing.\n",
        "\n",
        "The journey unfolds as follows:\n",
        "\n",
        "- Data Preprocessing: we'll start by converting lines of text into numerical tensors, making them machine-readable.\n",
        "\n",
        "- Dataset Creation: Next, we'll create a TensorFlow dataset, which will serve as the backbone for supplying data to our models.\n",
        "\n",
        "- Neural Network Training: Our models will be trained to predict the next set of characters, specifying the desired output length.\n",
        "\n",
        "- Character Embeddings: Character embeddings will be employed to represent each character as a vector, a fundamental technique in natural language processing.\n",
        "\n",
        "- GRU & LSTM Models: Our models utilizes a Gated Recurrent Unit (GRU) and Long Short Term Memory model (LSTM) to process character embeddings and make sequential predictions. T\n",
        "\n",
        "- Prediction Process: The model's predictions are achieved through a linear layer and log-softmax computation.\n",
        "\n",
        "This overview sets the stage for our exploration of text generation. Get ready to unravel the secrets of language and embark on a journey into the realm of creative writing and predictive text generation.\n",
        "\n",
        "And as usual let's start by importing all the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c0239115",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c0239115",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import traceback\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "30be7095",
      "metadata": {
        "deletable": false,
        "editable": false,
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30be7095",
        "outputId": "390d7828-e612-4989-ad83-ae9a76ea2021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n"
          ]
        }
      ],
      "source": [
        "dirname = './'\n",
        "filename = 'shakespeare_data.txt'\n",
        "lines = [] # storing all the lines in a variable.\n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:\n",
        "        # remove leading and trailing whitespace\n",
        "        pure_line = line.strip()#.lower()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)\n",
        "\n",
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53dc19a",
      "metadata": {
        "id": "b53dc19a"
      },
      "source": [
        "Let's examine a few lines from the corpus. Pay close attention to the structure and style employed by Shakespeare in this excerpt. Observe that character names are written in uppercase, and each line commences with a capital letter. Your task in this exercise is to construct a generative model capable of emulating this particular structural style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3eddfa33",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eddfa33",
        "outputId": "9d32c5a5-607d-4d39-ce75-b8a80e133ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BENVOLIO\tHere were the servants of your adversary,\n",
            "And yours, close fighting ere I did approach:\n",
            "I drew to part them: in the instant came\n",
            "The fiery Tybalt, with his sword prepared,\n",
            "Which, as he breathed defiance to my ears,\n",
            "He swung about his head and cut the winds,\n",
            "Who nothing hurt withal hiss'd him in scorn:\n",
            "While we were interchanging thrusts and blows,\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(lines[506:514]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b630ad99",
      "metadata": {
        "id": "b630ad99"
      },
      "source": [
        "\n",
        "### 1.2 - Create the vocabulary\n",
        "\n",
        "In the following code cell, we will create the vocabulary for text processing. The vocabulary is a crucial component for understanding and processing text data. Here's what the code does:\n",
        "\n",
        "- Concatenate all the lines in our dataset into a single continuous text, separated by line breaks.\n",
        "\n",
        "- Identify and collect the unique characters that make up the text. This forms the basis of our vocabulary.\n",
        "\n",
        "- To enhance the vocabulary, introduce two special characters:\n",
        "\n",
        "  - [UNK]: This character represents any unknown or unrecognized characters in the text.\n",
        "  - \"\" (empty character): This character is used for padding sequences when necessary.\n",
        "- The code concludes with the display of statistics, showing the total count of unique characters in the vocabulary and providing a visual representation of the complete character set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e656162",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e656162",
        "outputId": "18206afa-be46-4e6d-eaec-d21961a1e0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82 unique characters\n",
            "[UNK]  \t \n",
            "   ! $ & ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z |\n"
          ]
        }
      ],
      "source": [
        "text = \"\\n\".join(lines)\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "vocab.insert(0,\"[UNK]\") # Add a special character for any unknown\n",
        "vocab.insert(1,\"\") # Add the empty character for padding.\n",
        "\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print(\" \".join(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b2e15b",
      "metadata": {
        "id": "04b2e15b"
      },
      "source": [
        "\n",
        "### 1.3 - Convert a Line to Tensor\n",
        "\n",
        "Now that we have our list of lines, we will convert each character in that list to a number using the order given by our vocabulary. we can use [`tf.strings.unicode_split`](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_split) to split the text into characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f33c93a7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f33c93a7",
        "outputId": "7aacce8c-ca71-4677-bf1d-73f704c1461f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'H' b'e' b'l' b'l' b'o' b' ' b'w' b'o' b'r' b'l' b'd' b'!'], shape=(12,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "line = \"Hello world!\"\n",
        "chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7e5009",
      "metadata": {
        "id": "5d7e5009"
      },
      "source": [
        "Using your vocabulary, you can convert the characters given by `unicode_split` into numbers. The number will be the index of the character in the given vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bbe118d3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe118d3",
        "outputId": "debe0f5e-0996-4cbc-8515-c15e1b717d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "59\n",
            "63\n",
            "69\n",
            "75\n",
            "4\n",
            "16\n",
            "17\n"
          ]
        }
      ],
      "source": [
        "print(vocab.index('a'))\n",
        "print(vocab.index('e'))\n",
        "print(vocab.index('i'))\n",
        "print(vocab.index('o'))\n",
        "print(vocab.index('u'))\n",
        "print(vocab.index(' '))\n",
        "print(vocab.index('2'))\n",
        "print(vocab.index('3'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b45b0d",
      "metadata": {
        "id": "e9b45b0d"
      },
      "source": [
        "Tensorflow has a function [`tf.keras.layers.StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup)  that does this efficiently for list of characters. Note that the output object is of type `tf.Tensor`. Here is the result of applying the StringLookup function to the characters of \"Hello world\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f0e35746",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0e35746",
        "outputId": "37094463-b80b-4d68-b7fb-50e1e1ae8b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  4 77 69 72 66 58  5], shape=(12,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eadd334",
      "metadata": {
        "id": "4eadd334"
      },
      "source": [
        "\n",
        "### 1 - line_to_tensor\n",
        "\n",
        "Alright! now that we know what to do, let's wrap it into a function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0ef20200",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "0ef20200"
      },
      "outputs": [],
      "source": [
        "\n",
        "def line_to_tensor(line, vocab):\n",
        "    \"\"\"\n",
        "    Converts a line of text into a tensor of integer values representing characters.\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        vocab (list): A list containing the vocabulary of unique characters.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor(dtype=int64): A tensor containing integers (unicode values) corresponding to the characters in the `line`.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Split the input line into individual characters\n",
        "    chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "    # Map characters to their respective integer values using StringLookup\n",
        "    ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
        "\n",
        "\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e4200ace",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4200ace",
        "outputId": "68d599a1-7e32-49bc-fbe7-62e592b637c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [55 56 57  4 78 79 80]\n",
            "Output type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ],
      "source": [
        "# Test the function\n",
        "tmp_ids = line_to_tensor('abc xyz', vocab)\n",
        "print(f\"Result: {tmp_ids}\")\n",
        "print(f\"Output type: {type(tmp_ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1a55d7",
      "metadata": {
        "id": "8b1a55d7"
      },
      "source": [
        "we will also need a function that produces text given a numeric tensor. This function will be useful for inspection when we use your models to generate new text, because we will be able to see words rather than lists of numbers. The function will use the inverse Lookup function `tf.keras.layers.StringLookup` with `invert=True` in its parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa56d848",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "fa56d848"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids, vocab):\n",
        "    \"\"\"\n",
        "    Converts a tensor of integer values into human-readable text.\n",
        "\n",
        "    Args:\n",
        "        ids (tf.Tensor): A tensor containing integer values (unicode IDs).\n",
        "        vocab (list): A list containing the vocabulary of unique characters.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the characters in human-readable format.\n",
        "    \"\"\"\n",
        "    # Initialize the StringLookup layer to map integer IDs back to characters\n",
        "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
        "\n",
        "    # Use the layer to decode the tensor of IDs into human-readable text\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242da763",
      "metadata": {
        "id": "242da763"
      },
      "source": [
        "Use the function for decoding the tensor produced by \"Hello world!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "724687d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "724687d2",
        "outputId": "d6666dc8-3af6-496e-f3f4-005b30bc063e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello world!'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "text_from_ids(ids, vocab).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd621c9d",
      "metadata": {
        "id": "fd621c9d"
      },
      "source": [
        "\n",
        "### 1.4 - Prepare our data for training and testing\n",
        "As usual, we will need some data for training our model, and some data for testing its performance. So, we will use 124097 lines for training and 1000 lines for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "771ec0a6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "771ec0a6",
        "outputId": "829c1654-bc13-4b23-bffd-4dfc03836017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training lines: 124097\n",
            "Number of validation lines: 1000\n"
          ]
        }
      ],
      "source": [
        "train_lines = lines[:-1000] # Leave the rest for training\n",
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "\n",
        "print(f\"Number of training lines: {len(train_lines)}\")\n",
        "print(f\"Number of validation lines: {len(eval_lines)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59f4b44",
      "metadata": {
        "id": "c59f4b44"
      },
      "source": [
        "\n",
        "### 1.5 - TensorFlow dataset\n",
        "\n",
        "Most of the time in Natural Language Processing, and AI in general we use batches when training our models. Here, we will build a dataset that takes in some text and returns a batch of text fragments (Not necesarly full sentences) that you will use for training.\n",
        "\n",
        "- The generator will produce text fragments encoded as numeric tensors of a desired length (batches)\n",
        "\n",
        "Once we create the dataset, we can iterate on it like this:\n",
        "\n",
        "```\n",
        "data_generator.take(1)\n",
        "```\n",
        "\n",
        "This generator returns the data in a format that we could directly use in your model when computing the feed-forward of our algorithm. This batch dataset generator returns batches of data in an endless way.\n",
        "\n",
        "In order to get a dataset generator that produces batches of fragments from the corpus, we first need to convert the whole text into a single line, and then transform it into a single big tensor. This is only possible if our data fits completely into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aa72ed77",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa72ed77",
        "outputId": "537b37bd-8cfc-4b75-815e-69b27f913fbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(26,), dtype=int64, numpy=\n",
              "array([34, 59, 66, 66, 69,  4, 77, 69, 72, 66, 58,  5,  3, 33, 59, 68, 59,\n",
              "       72, 55, 74, 63, 76, 59,  4, 27, 35])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "all_ids = line_to_tensor(\"\\n\".join([\"Hello world!\", \"Generative AI\"]), vocab)\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f00e068",
      "metadata": {
        "id": "2f00e068"
      },
      "source": [
        "Create a dataset out of a tensor like input. This initial dataset will dispatch numbers in packages of a specified length. For example, we can use it for getting the 10 first encoded characters of our dataset. To make it easier to read, we can use the `text_from_ids` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "375dc450",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "375dc450",
        "outputId": "cdab87e3-0aa7-42d8-9b8f-f1dcf045376f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'H', b'e', b'l', b'l', b'o', b' ', b'w', b'o', b'r', b'l']\n"
          ]
        }
      ],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print([text_from_ids([ids], vocab).numpy() for ids in ids_dataset.take(10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad1d4a1",
      "metadata": {
        "id": "2ad1d4a1"
      },
      "source": [
        "But we can configure this dataset to produce batches of the same size each time. We could use this functionality to produce text fragments of a desired size (`seq_length + 1`). We will explain later why we need an extra character into the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1903124a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1903124a"
      },
      "outputs": [],
      "source": [
        "seq_length = 10\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f8f14d",
      "metadata": {
        "id": "f2f8f14d"
      },
      "source": [
        "we can verify that the data generator produces encoded fragments of text of the desired length. For example, let's ask the generator to produce 2 batches of data using the function `data_generator.take(2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "66bffbd8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66bffbd8",
        "outputId": "a2198f00-a207-4cf2-fd08-40061da6ccfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  4 77 69 72 66 58], shape=(11,), dtype=int64)\n",
            "tf.Tensor([ 5  3 33 59 68 59 72 55 74 63 76], shape=(11,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for seq in data_generator.take(2):\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daab16e5",
      "metadata": {
        "id": "daab16e5"
      },
      "source": [
        "But as usual, it is easier to understand if we print it in human readable characters using the 'text_from_ids' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fd881a9a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd881a9a",
        "outputId": "ab17c184-a5f7-4244-eeb3-1de2c5579282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. b'Hello world'\n",
            "2. b'!\\nGenerativ'\n"
          ]
        }
      ],
      "source": [
        "i = 1\n",
        "for seq in data_generator.take(2):\n",
        "    print(f\"{i}. {text_from_ids(seq, vocab).numpy()}\")\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa96d87a",
      "metadata": {
        "id": "fa96d87a"
      },
      "source": [
        "\n",
        "### 1.6 - Create the input and the output for your model\n",
        "\n",
        "In this part we have to predict the next character in a sequence. The following function creates 2 tensors, each with a length of `seq_length` out of the input sequence of lenght `seq_length + 1`. The first one contains the first `seq_length` elements and the second one contains the last `seq_length` elements. For example, if you split the sequence `['H', 'e', 'l', 'l', 'o']`, we will obtain the sequences `['H', 'e', 'l', 'l']` and `['e', 'l', 'l', 'o']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "066aef3c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "066aef3c"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    \"\"\"\n",
        "    Splits the input sequence into two sequences, where one is shifted by one position.\n",
        "\n",
        "    Args:\n",
        "        sequence (tf.Tensor or list): A list of characters or a tensor.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor, tf.Tensor: Two tensors representing the input and output sequences for the model.\n",
        "    \"\"\"\n",
        "    # Create the input sequence by excluding the last character\n",
        "    input_text = sequence[:-1]\n",
        "    # Create the target sequence by excluding the first character\n",
        "    target_text = sequence[1:]\n",
        "\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53249ab",
      "metadata": {
        "id": "b53249ab"
      },
      "source": [
        "Look the result using the following sequence of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4f1439bc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f1439bc",
        "outputId": "beaf38bb-d489-43ab-fc98-ca829932bcd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c4512b",
      "metadata": {
        "id": "14c4512b"
      },
      "source": [
        "The first sequence will be the input and the second sequence will be the expected output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1546c91",
      "metadata": {
        "id": "a1546c91"
      },
      "source": [
        "Now, put all this together into a function to create your batch dataset generator\n",
        "\n",
        "\n",
        "### 2 - create_batch_dataset\n",
        "\n",
        "\n",
        "- Join all the input lines into a single string. When we have a big dataset, we would better use a flow from directory or any other kind of generator.\n",
        "- Transform the input text into numeric tensors\n",
        "- Create a TensorFlow DataSet from our numeric tensors: Just feed the numeric tensors into the function [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)\n",
        "- Make the dataset produce batches of data that will form a single sample each time. This is, make the dataset produce a sequence of `seq_length + 1`, rather than single numbers at each time. we can do it using the `batch` function of the already created dataset. we must specify the length of the produced sequences (`seq_length + 1`). So, the sequence length produced by the dataset will `seq_length + 1`. It must have that extra element since you will get the input and the output sequences out of the same element. `drop_remainder=True` will drop the sequences that do not have the required length. This could happen each time that the dataset reaches the end of the input sequence.\n",
        "- Use the `split_input_target` to split each element produced by the dataset into the mentioned input and output sequences.The input will have the first `seq_length` elements, and the output will have the last `seq_length`. So, after this step, the dataset generator will produce batches of pairs (input, output) sequences.\n",
        "- Create the final dataset, using `dataset_xy` as the starting point. we will configure this dataset to shuffle the data during the generation of the data with the specified BUFFER_SIZE. For performance reasons, we would like that tensorflow pre-process the data in parallel with training. That is called [`prefetching`](https://www.tensorflow.org/guide/data_performance#prefetching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0d736de0",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "0d736de0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
        "    \"\"\"\n",
        "    Creates a batch dataset from a list of text lines.\n",
        "\n",
        "    Args:\n",
        "        lines (list): A list of strings with the input data, one line per row.\n",
        "        vocab (list): A list containing the vocabulary.\n",
        "        seq_length (int): The desired length of each sample.\n",
        "        batch_size (int): The batch size.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A batch dataset generator.\n",
        "    \"\"\"\n",
        "    # Buffer size to shuffle the dataset\n",
        "    # (TF data is designed to work with possibly infinite sequences,\n",
        "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    # it maintains a buffer in which it shuffles elements).\n",
        "    BUFFER_SIZE = 10000\n",
        "\n",
        "    # For simplicity, just join all lines into a single line\n",
        "    single_line_data  = \"\\n\".join(lines)\n",
        "\n",
        "    # Convert your data into a tensor using the given vocab\n",
        "    all_ids = line_to_tensor(single_line_data, vocab)\n",
        "    # Create a TensorFlow dataset from the data tensor\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "    # Create a batch dataset\n",
        "    data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "    # Map each input sample using the split_input_target function\n",
        "    dataset_xy = data_generator.map(split_input_target)\n",
        "\n",
        "    # Assemble the final dataset with shuffling, batching, and prefetching\n",
        "    dataset = (\n",
        "        dataset_xy\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8b4821f2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4821f2",
        "outputId": "8e86350a-93b3-4599-fd44-588f3c0ffe2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prints the elements into a single batch. The batch contains 2 elements: \n",
            "\n",
            "\u001b[94mInput0\t: b'and sight distra'\n",
            "\n",
            "\u001b[93mTarget0\t: b'nd sight distrac'\n",
            "\n",
            "\n",
            "\u001b[94mInput1\t: b'when in his fair'\n",
            "\n",
            "\u001b[93mTarget1\t: b'hen in his fair '\n"
          ]
        }
      ],
      "source": [
        "# test the function\n",
        "tf.random.set_seed(1)\n",
        "dataset = create_batch_dataset(train_lines[1:100], vocab, seq_length=16, batch_size=2)\n",
        "\n",
        "print(\"Prints the elements into a single batch. The batch contains 2 elements: \")\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"\\n\\033[94mInput0\\t:\", text_from_ids(input_example[0], vocab).numpy())\n",
        "    print(\"\\n\\033[93mTarget0\\t:\", text_from_ids(target_example[0], vocab).numpy())\n",
        "\n",
        "    print(\"\\n\\n\\033[94mInput1\\t:\", text_from_ids(input_example[1], vocab).numpy())\n",
        "    print(\"\\n\\033[93mTarget1\\t:\", text_from_ids(target_example[1], vocab).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272a7e47",
      "metadata": {
        "id": "272a7e47"
      },
      "source": [
        "\n",
        "### 1.7 - Create the training dataset\n",
        "\n",
        "Now, we can generate your training dataset using the functions defined above. This will produce pairs of input/output tensors each time the batch generator creates an entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b4d4f89f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b4d4f89f"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "dataset = create_batch_dataset(train_lines, vocab, seq_length=100, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d941b6",
      "metadata": {
        "id": "d9d941b6"
      },
      "source": [
        "\n",
        "## 2 - Defining the GRU Language Model (GRULM) & LSTM Language Model (LSTMLM)\n",
        "\n",
        "Now that we have the input and output tensors, we will go ahead and initialize our models. We will be implementing the `GRULM` & `LSTMLM`, To implement this model, we will be using `TensorFlow`. Instead of implementing the `GRU` & `LSTM` from scratch, we will use the necessary methods from a built-in package. You can use the following packages when constructing the model:\n",
        "\n",
        "- `tf.keras.layers.Embedding`: Initializes the embedding. In this case it is the size of the vocabulary by the dimension of the model. [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)  \n",
        "    - `Embedding(vocab_size, embedding_dim)`.\n",
        "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
        "    - `embedding_dim` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
        "___\n",
        "\n",
        "- `tf.keras.layers.GRU`: `TensorFlow` GRU layer. [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)) Builds a traditional GRU of rnn_units with dense internal transformations. You can read the paper here: https://arxiv.org/abs/1412.3555\n",
        "    - `units`: Number of recurrent units in the layer. It must be set to `rnn_units`\n",
        "    - `return_sequences`: It specifies if the model returns a sequence of predictions. Set it to `True`\n",
        "    - `return_state`: It specifies if the model must return the last internal state along with the prediction. Set it to `True`\n",
        "___\n",
        "- `tf.keras.layers.LSTM`: `TensorFlow` LSTM layer. [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)) Builds a traditional LSTM of rnn_units with dense internal transformations.\n",
        "    - `units`: Number of recurrent units in the layer. It must be set to `rnn_units`\n",
        "    - `return_sequences`: It specifies if the model returns a sequence of predictions. Set it to `True`\n",
        "    - `return_state`: It specifies if the model must return the last internal state along with the prediction. Set it to `True`\n",
        "___\n",
        "\n",
        "- `tf.keras.layers.Dense`: A dense layer. [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). You must set the following parameters:\n",
        "    - `units`: Number of units in the layer. It must be set to `vocab_size`\n",
        "    - `activation`: It must be set to `log_softmax` function as described in the next line.\n",
        "___\n",
        "\n",
        "- `tf.nn.log_softmax`: Log of the output probabilities. [docs](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax)\n",
        "    - You don't need to set any parameters, just set the activation parameter as `activation=tf.nn.log_softmax`.\n",
        "___\n",
        "\n",
        "\n",
        "###  3.1 - GRULM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "37306da8",
      "metadata": {
        "deletable": false,
        "id": "37306da8",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "class GRULM(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A GRU-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
        "        rnn_units (int, optional): Number of units in the GRU cell. Defaults to 128.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: A GRULM language model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units = 128):\n",
        "        super().__init__(self)\n",
        "\n",
        "        # Create an embedding layer to map token indices to embedding vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define a GRU (Gated Recurrent Unit) layer for sequence modeling\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        # Apply a dense layer with log-softmax activation to predict next tokens\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size,activation=tf.nn.log_softmax)\n",
        "\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        # Map input tokens to embedding vectors\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            # Get initial state from the GRU layer\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        # Predict the next tokens and apply log-softmax activation\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "169693f5",
      "metadata": {
        "id": "169693f5"
      },
      "outputs": [],
      "source": [
        "class LSTMLM(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A LSTM-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
        "        rnn_units (int, optional): Number of units in the GRU cell. Defaults to 128.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: A LSTMLM language model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units = 128):\n",
        "        super().__init__(self)\n",
        "\n",
        "\n",
        "\n",
        "        # Create an embedding layer to map token indices to embedding vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define a LSTM layer for sequence modeling\n",
        "        self.LSTM = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "        # Apply a dense layer with log-softmax activation to predict next tokens\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size,activation=tf.nn.log_softmax)\n",
        "\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        # Map input tokens to embedding vectors\n",
        "        x = self.embedding(x, training=training)\n",
        "\n",
        "        if states is None:\n",
        "            # If no initial states, let the LSTM layer create them\n",
        "            outputs, state_h, state_c = self.LSTM(x, training=training)\n",
        "        else:\n",
        "        # If initial states are provided, use them\n",
        "            outputs, state_h, state_c = self.LSTM(x, initial_state=states, training=training)\n",
        "\n",
        "        # Predict the next tokens and apply log-softmax activation\n",
        "        x = self.dense(outputs, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            # Return the output and states if return_state is True\n",
        "            return x, [state_h, state_c]\n",
        "        else:\n",
        "            # Only return the output otherwise\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4e2e4c",
      "metadata": {
        "id": "2b4e2e4c"
      },
      "source": [
        "Now, you can define a new GRULM & LSTMLM models. we must set the `vocab_size` to 82; the size of the embedding `embedding_dim` to 256, and the number of units that will have our recurrent neural network `rnn_units` to 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "97558190",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "97558190"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "\n",
        "vocab_size = 82\n",
        "\n",
        "# The embedding dimension\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN layers\n",
        "\n",
        "rnn_units = 512\n",
        "\n",
        "model_gru = GRULM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "dd53e9ac",
      "metadata": {
        "id": "dd53e9ac"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "\n",
        "vocab_size = 82\n",
        "\n",
        "# The embedding dimension\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN layers\n",
        "\n",
        "rnn_units = 512\n",
        "\n",
        "\n",
        "model_lstm = LSTMLM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "facb51a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "facb51a7",
        "outputId": "3592d941-9764-4ebe-bd6e-61a25af2d7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"grulm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 256)          20992     \n",
            "                                                                 \n",
            " gru (GRU)                   [(None, 100, 512),        1182720   \n",
            "                              (None, 512)]                       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100, 82)           42066     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1245778 (4.75 MB)\n",
            "Trainable params: 1245778 (4.75 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# testing our model\n",
        "\n",
        "try:\n",
        "    # Simulate inputs of length 100. This allows to compute the shape of all inputs and outputs of our network\n",
        "    model_gru.build(input_shape=(BATCH_SIZE, 100))\n",
        "    model_gru.call(Input(shape=(100)))\n",
        "    model_gru.summary()\n",
        "except:\n",
        "    print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization\\n\\n\")\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dce9b23e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce9b23e",
        "outputId": "97b4b112-9878-4239-9e82-d8099c3ee9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstmlm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 256)          20992     \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 100, 512),        1574912   \n",
            "                              (None, 512),                       \n",
            "                              (None, 512)]                       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100, 82)           42066     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1637970 (6.25 MB)\n",
            "Trainable params: 1637970 (6.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# testing our model\n",
        "\n",
        "\n",
        "try:\n",
        "    # Simulate inputs of length 100. This allows to compute the shape of all inputs and outputs of our network\n",
        "\n",
        "\n",
        "    model_lstm.build(input_shape=(BATCH_SIZE, 100))\n",
        "    model_lstm.call(Input(shape=(100)))\n",
        "    model_lstm.summary()\n",
        "except:\n",
        "    print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization \\n\\n\")\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620324ba",
      "metadata": {
        "id": "620324ba"
      },
      "source": [
        "Now, let's use the GRU model for predicting the next character using the untrained model. At the begining the model will generate only gibberish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cbf5745e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbf5745e",
        "outputId": "856c5253-be08-4ae8-b872-f0077b11ab7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [55  4 57 55 75 73 59 66 59 73 73  4 60 55 68 74 55 73 79 11  3 27 68 58\n",
            "  4 57 62 63 66 58 63 73 62  4 59 72 72 69 72 11  4 74 62 55 74  4 74 62\n",
            " 59 79  4 55 72 59  4 55 60 72 55 63 58 25  3 28 63 58 73  4 74 62 59 67\n",
            "  4 66 59 55 76 59  4 71 75 55 65 63 68 61 11  4 56 63 58 73  4 74 62 59\n",
            " 67  4 60 59]\n",
            "\n",
            " (1, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
        "    example_batch_predictions = model_gru(tf.constant([input_example_batch[0].numpy()]))\n",
        "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fec38a",
      "metadata": {
        "id": "f4fec38a"
      },
      "source": [
        "Let's try the LSTM model as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9d7e9a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d7e9a34",
        "outputId": "d19775dd-4b1d-47db-a3ce-c531d7ef8168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [63 58  4 62 59 72 59 11  3 27  4 72 63 68 61  4 63 68  4 57 62 55 73 59\n",
            "  4 69 60  4 79 69 75 24  4 73 69  4 58 63 58  4 35  4 55 56 75 73 59  3\n",
            " 39 79 73 59 66 60 11  4 67 79  4 73 59 72 76 55 68 74  4 55 68 58 11  4\n",
            " 35  4 60 59 55 72  4 67 59 11  4 79 69 75 24  3 47 68 58 59 72  4 79 69\n",
            " 75 72  4 62]\n",
            "\n",
            " (1, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
        "    example_batch_predictions = model_lstm(tf.constant([input_example_batch[0].numpy()]))\n",
        "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96266a51",
      "metadata": {
        "id": "96266a51"
      },
      "source": [
        "The output size is (1, 100, 82). We predicted only on the first sequence generated by the batch generator. 100 is the number of predicted characters. It has exactly the same length as the input. And there are 82 values for each predicted character. Each of these 82 real values are related to the logarithm likelihood of each character to be the next one in the sequence. The bigger the value, the higher the likelihood. As the network is not trained yet, all those values must be very similar and random. Just check the values for the last prediction on the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7df09c7b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7df09c7b",
        "outputId": "b17b2385-35e7-4d57-a8e4-baf8f87b5bb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.419118 , -4.412833 , -4.414311 , -4.3991838, -4.401638 ,\n",
              "       -4.3973403, -4.402481 , -4.422451 , -4.3978863, -4.400891 ,\n",
              "       -4.416775 , -4.4076934, -4.3987746, -4.4216504, -4.3982496,\n",
              "       -4.4023438, -4.405349 , -4.4075117, -4.406863 , -4.416502 ,\n",
              "       -4.4014177, -4.40884  , -4.3937726, -4.4107   , -4.41072  ,\n",
              "       -4.4062586, -4.4070373, -4.4079137, -4.4231052, -4.401702 ,\n",
              "       -4.399208 , -4.4121933, -4.404532 , -4.4031453, -4.4102764,\n",
              "       -4.3956776, -4.394757 , -4.425828 , -4.4202027, -4.4102793,\n",
              "       -4.4014764, -4.405326 , -4.408773 , -4.403935 , -4.4135666,\n",
              "       -4.4065323, -4.4056115, -4.4036484, -4.403766 , -4.406415 ,\n",
              "       -4.406102 , -4.40142  , -4.418734 , -4.4140863, -4.397257 ,\n",
              "       -4.4230905, -4.41517  , -4.4022083, -4.4033437, -4.4042015,\n",
              "       -4.408529 , -4.3947854, -4.394715 , -4.4031367, -4.4010143,\n",
              "       -4.401338 , -4.403673 , -4.408378 , -4.403745 , -4.4024844,\n",
              "       -4.4252534, -4.4116216, -4.400852 , -4.416922 , -4.397103 ,\n",
              "       -4.40431  , -4.424171 , -4.3950915, -4.404602 , -4.4057035,\n",
              "       -4.3991327, -4.4049315], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "example_batch_predictions[0][99].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ff6fac",
      "metadata": {
        "id": "96ff6fac"
      },
      "source": [
        "And the simplest way to choose the next character is by getting the index of the element with the highest likelihood. So, for instance, the prediction for the last characeter would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "90eae11a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90eae11a",
        "outputId": "cad4de88-619c-40b3-d309-e6cf198e8aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "last_character = tf.math.argmax(example_batch_predictions[0][99])\n",
        "print(last_character.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398a7602",
      "metadata": {
        "id": "398a7602"
      },
      "source": [
        "And the prediction for the whole sequence would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6e62ea06",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e62ea06",
        "outputId": "b8d55f34-9d8f-4f19-865e-f1f7711636f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32 71 80 80 54  3 54 11 43 80 80 69 69 69 69 50 50 69 69 34 14 14 20 34\n",
            " 34 34 40 61 69 40 43 40 43 68 68  5 61 68 71 61 61 61 61 74 59 20 42 47\n",
            " 47 77  5 47 47 40 48 64 43 63 43 64 34 22 22 20 69 34 34 34 34 34 71 16\n",
            " 24 43 61 72 34 69 69 69 22 35 43 69 35 35 35 35 35 35 42 42 22 16 77 77\n",
            " 77 22 22 22]\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.math.argmax(example_batch_predictions[0], axis=1)\n",
        "print(sampled_indices.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10564aee",
      "metadata": {
        "id": "10564aee"
      },
      "source": [
        "Those 100 numbers represent 100 predicted characters. However, humans cannot read this. So, let's print the input and output sequences using our `text_from_ids` function, to check what is going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ff501624",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff501624",
        "outputId": "60ada0a7-9565-4627-c04c-beacf11ca693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tf.Tensor(b'id here,\\nA ring in chase of you: so did I abuse\\nMyself, my servant and, I fear me, you:\\nUnder your h', shape=(), dtype=string)\n",
            "\n",
            "Next Char Predictions:\n",
            " tf.Tensor(b'Fqzz]\\n],QzzooooXXooH006HHHNgoNQNQnn!gnqggggte6PUUw!UUNVjQiQjH886oHHHHHq2:QgrHooo8IQoIIIIIIPP82www888', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0], vocab))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices, vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d090f1a",
      "metadata": {
        "id": "8d090f1a"
      },
      "source": [
        "As expected, the untrained model just produces random text as response of the given input. It is also important to note that getting the index of the maximum score is not always the best choice. In the last part of the notebook we will see another way to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aeb3f3",
      "metadata": {
        "id": "97aeb3f3"
      },
      "source": [
        "\n",
        "## 3 - Training\n",
        "\n",
        "Now we are going to train our models. As usual, we have to define the cost function and the optimizer. We will use the following built-in functions provided by TensorFlow:\n",
        "\n",
        "- [`tf.losses.SparseCategoricalCrossentropy()`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy): The Sparce Categorical Cross Entropy loss. It is the loss function used for multiclass classification.    \n",
        "    - `from_logits=True`: This parameter informs the loss function that the output values generated by the model are not normalized like a probability distribution. This is our case, since our GRULM model uses a `log_softmax` activation rather than the `softmax`.\n",
        "- [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam): Use Adaptive Moment Estimation, a stochastic gradient descent method optimizer that works well in most of the cases. We will set the `learning_rate` to 0.00125.\n",
        "\n",
        "\n",
        "###  4 - compile_models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5c2948bb",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "5c2948bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Sets the loss and optimizer for the given model\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model to compile.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled model.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Define the loss function. Use SparseCategoricalCrossentropy\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True)\n",
        "    # Define and Adam optimizer\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate= 0.00125)\n",
        "    # Compile the model using the parametrized Adam optimizer and the SparseCategoricalCrossentropy funcion\n",
        "    model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "343ab5e2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "343ab5e2",
        "outputId": "85127222-0248-4693-a77b-41eecaad149a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "790/790 [==============================] - 24s 24ms/step - loss: 2.0102\n",
            "Epoch 2/30\n",
            "790/790 [==============================] - 19s 22ms/step - loss: 1.4798\n",
            "Epoch 3/30\n",
            "790/790 [==============================] - 21s 24ms/step - loss: 1.3803\n",
            "Epoch 4/30\n",
            "790/790 [==============================] - 24s 24ms/step - loss: 1.3347\n",
            "Epoch 5/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.3055\n",
            "Epoch 6/30\n",
            "790/790 [==============================] - 19s 23ms/step - loss: 1.2852\n",
            "Epoch 7/30\n",
            "790/790 [==============================] - 21s 24ms/step - loss: 1.2696\n",
            "Epoch 8/30\n",
            "790/790 [==============================] - 20s 24ms/step - loss: 1.2566\n",
            "Epoch 9/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2458\n",
            "Epoch 10/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2368\n",
            "Epoch 11/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2289\n",
            "Epoch 12/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2218\n",
            "Epoch 13/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2155\n",
            "Epoch 14/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2102\n",
            "Epoch 15/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.2054\n",
            "Epoch 16/30\n",
            "790/790 [==============================] - 20s 24ms/step - loss: 1.2010\n",
            "Epoch 17/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1976\n",
            "Epoch 18/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1940\n",
            "Epoch 19/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1911\n",
            "Epoch 20/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1883\n",
            "Epoch 21/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1857\n",
            "Epoch 22/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1837\n",
            "Epoch 23/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1822\n",
            "Epoch 24/30\n",
            "790/790 [==============================] - 21s 23ms/step - loss: 1.1799\n",
            "Epoch 25/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1787\n",
            "Epoch 26/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1777\n",
            "Epoch 27/30\n",
            "790/790 [==============================] - 21s 24ms/step - loss: 1.1760\n",
            "Epoch 28/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1749\n",
            "Epoch 29/30\n",
            "790/790 [==============================] - 20s 23ms/step - loss: 1.1738\n",
            "Epoch 30/30\n",
            "790/790 [==============================] - 20s 24ms/step - loss: 1.1735\n"
          ]
        }
      ],
      "source": [
        "# GRULM\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "# Compile the model\n",
        "model_gru = compile_model(model_gru)\n",
        "# Fit the model\n",
        "history_gru = model_gru.fit(dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d786e54e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d786e54e",
        "outputId": "81c849a5-d50c-4997-bc9e-a8ad2ee22ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "790/790 [==============================] - 26s 28ms/step - loss: 2.0635\n",
            "Epoch 2/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.5645\n",
            "Epoch 3/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.4366\n",
            "Epoch 4/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.3761\n",
            "Epoch 5/30\n",
            "790/790 [==============================] - 23s 28ms/step - loss: 1.3391\n",
            "Epoch 6/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.3130\n",
            "Epoch 7/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2936\n",
            "Epoch 8/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2776\n",
            "Epoch 9/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2644\n",
            "Epoch 10/30\n",
            "790/790 [==============================] - 24s 27ms/step - loss: 1.2531\n",
            "Epoch 11/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.2428\n",
            "Epoch 12/30\n",
            "790/790 [==============================] - 23s 28ms/step - loss: 1.2339\n",
            "Epoch 13/30\n",
            "790/790 [==============================] - 23s 28ms/step - loss: 1.2260\n",
            "Epoch 14/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2186\n",
            "Epoch 15/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2116\n",
            "Epoch 16/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.2053\n",
            "Epoch 17/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.1993\n",
            "Epoch 18/30\n",
            "790/790 [==============================] - 28s 33ms/step - loss: 1.1941\n",
            "Epoch 19/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.1887\n",
            "Epoch 20/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.1838\n",
            "Epoch 21/30\n",
            "790/790 [==============================] - 24s 27ms/step - loss: 1.1790\n",
            "Epoch 22/30\n",
            "790/790 [==============================] - 24s 27ms/step - loss: 1.1746\n",
            "Epoch 23/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.1704\n",
            "Epoch 24/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.1663\n",
            "Epoch 25/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.1626\n",
            "Epoch 26/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.1589\n",
            "Epoch 27/30\n",
            "790/790 [==============================] - 24s 27ms/step - loss: 1.1553\n",
            "Epoch 28/30\n",
            "790/790 [==============================] - 24s 27ms/step - loss: 1.1520\n",
            "Epoch 29/30\n",
            "790/790 [==============================] - 23s 27ms/step - loss: 1.1489\n",
            "Epoch 30/30\n",
            "790/790 [==============================] - 24s 28ms/step - loss: 1.1458\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "# Compile the model\n",
        "model_lstm = compile_model(model_lstm)\n",
        "# Fit the model\n",
        "history_lstm = model_lstm.fit(dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**let's plot the loss curves of the two models**\n"
      ],
      "metadata": {
        "id": "Xh352MX1CUmH"
      },
      "id": "Xh352MX1CUmH"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss values\n",
        "loss_gru = history_gru.history['loss']\n",
        "loss_lstm = history_lstm.history['loss']\n",
        "\n",
        "# Define the range of epochs\n",
        "epochs = range(1, EPOCHS + 1)\n",
        "\n",
        "# Create a plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot GRU loss\n",
        "plt.plot(epochs, loss_gru, label='GRU Loss', marker='o')\n",
        "\n",
        "# Plot LSTM loss\n",
        "plt.plot(epochs, loss_lstm, label='LSTM Loss', marker='o')\n",
        "\n",
        "# Adding plot title and labels\n",
        "plt.title('GRU vs LSTM Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "mcYWjEDsCOhT",
        "outputId": "f090f4ed-c0e9-4162-f318-d88b1e05f754"
      },
      "id": "mcYWjEDsCOhT",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b86cd1dd",
      "metadata": {
        "id": "b86cd1dd"
      },
      "source": [
        "You can uncomment the following cell to save the weigthts of your model. This allows you to use the model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "30c3ead1",
      "metadata": {
        "deletable": false,
        "id": "30c3ead1"
      },
      "outputs": [],
      "source": [
        "# # If you want, you can save the final model. Here is deactivated.\n",
        "#output_dir = './your-model/'\n",
        "\n",
        "#try:\n",
        "#    shutil.rmtree(output_dir)\n",
        "#except OSError as e:\n",
        "#    pass\n",
        "\n",
        "#model.save_weights(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978febbb",
      "metadata": {
        "id": "978febbb"
      },
      "source": [
        "\n",
        "## 4 - Evaluation  \n",
        "\n",
        "### 4.1 - Evaluating using the Deep Nets\n",
        "\n",
        "To evaluate language models, we usually use perplexity which is a measure of how well a probability model predicts a sample. Note that perplexity is defined as:\n",
        "\n",
        "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
        "\n",
        "As an implementation hack, we would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient).\n",
        "\n",
        "\n",
        "$$\\log P(W) = {\\log\\left(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\right)}$$$$ = \\log\\left(\\left(\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}\\right)^{\\frac{1}{N}}\\right)$$\n",
        "$$ = \\log\\left(\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\right)^{-\\frac{1}{N}}\\right)$$$$ = -\\frac{1}{N}{\\log\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\right)} $$$$ = -\\frac{1}{N}{{\\sum_{i=1}^{N}{\\log P(w_i| w_1,...,w_{n-1})}}} $$\n",
        "\n",
        "\n",
        "### 5 - log_perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d31ed99d",
      "metadata": {
        "deletable": false,
        "id": "d31ed99d",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_perplexity(preds, target):\n",
        "    \"\"\"\n",
        "    Function to calculate the log perplexity of a model.\n",
        "\n",
        "    Args:\n",
        "        preds (tf.Tensor): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (tf.Tensor): Actual list of batches of tensors corresponding to lines of text.\n",
        "\n",
        "    Returns:\n",
        "        float: The log perplexity of the model.\n",
        "    \"\"\"\n",
        "    PADDING_ID = 1\n",
        "\n",
        "\n",
        "    # Calculate log probabilities for predictions using one-hot encoding\n",
        "    log_p = np.sum(preds * tf.one_hot(target, depth=preds.shape[-1]), axis= -1) # HINT: tf.one_hot() should replace one of the Nones\n",
        "    # Identify non-padding elements in the target\n",
        "    non_pad = 1.0 - np.equal(target, PADDING_ID)          # You should check if the target equals to PADDING_ID\n",
        "    # Apply non-padding mask to log probabilities to exclude padding\n",
        "    log_p = log_p * non_pad                             # Get rid of the padding\n",
        "    # Calculate the log perplexity by taking the sum of log probabilities and dividing by the sum of non-padding elements\n",
        "    log_ppx = np.sum(log_p, axis=-1) / np.sum(non_pad, axis=-1) # Remember to set the axis properly when summing up\n",
        "    # Compute the mean of log perplexity\n",
        "    log_ppx = np.mean(log_ppx) # Compute the mean of the previous expression\n",
        "\n",
        "\n",
        "    return -log_ppx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dfbc9e9",
      "metadata": {
        "id": "3dfbc9e9"
      },
      "source": [
        "Now, we will use the 1000 lines of the corpus that were reserved at the begining of this notebook as test data. We will apply the same preprocessing as you did for the train dataset: get the numeric tensor from the input lines, and use the `split_input_target` to generate the inputs and the expected outputs.\n",
        "\n",
        "Second, we will predict the next characters for the whole dataset, and we will compute the perplexity for the expected outputs and the given predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5104a2ed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5104a2ed",
        "outputId": "39f657cc-bb3c-427c-ff58-2ba1bdaa5929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log perplexity and perplexity of Our GRULM model are 1.21788068144193 and 3.3800168052226462 respectively\n",
            "The log perplexity and perplexity of Our LSTMLM model are 1.1904782262372597 and 3.288653551798703 respectively\n"
          ]
        }
      ],
      "source": [
        "#for line in eval_lines[1:3]:\n",
        "eval_text = \"\\n\".join(eval_lines)\n",
        "eval_ids = line_to_tensor([eval_text], vocab)\n",
        "input_ids, target_ids = split_input_target(tf.squeeze(eval_ids, axis=0))\n",
        "\n",
        "preds_gru, status_gru = model_gru(tf.expand_dims(input_ids, 0), training=False, states=None, return_state=True)\n",
        "preds_lstm, status_lstm = model_lstm(tf.expand_dims(input_ids, 0), training=False, states=None, return_state=True)\n",
        "#Get the log perplexity\n",
        "log_ppx_gru = log_perplexity(preds_gru, tf.expand_dims(target_ids, 0))\n",
        "log_ppx_lstm = log_perplexity(preds_lstm, tf.expand_dims(target_ids, 0))\n",
        "print(f'The log perplexity and perplexity of Our GRULM model are {log_ppx_gru} and {np.exp(log_ppx_gru)} respectively')\n",
        "print(f'The log perplexity and perplexity of Our LSTMLM model are {log_ppx_lstm} and {np.exp(log_ppx_lstm)} respectively')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b1fea0",
      "metadata": {
        "id": "50b1fea0"
      },
      "source": [
        "\n",
        "## 5 - Generating Language with our Own Models\n",
        "\n",
        "The GRULM & LSTMLM models demonstrate an impressive ability to predict the most likely characters in a sequence, based on log scores. However, it's important to acknowledge that these models, in its default form, is deterministic and can result in repetitive and monotonous outputs. For instance, it tends to provide the same answer to a question consistently.\n",
        "\n",
        "To make our language model more dynamic and versatile, we can introduce an element of randomness into its predictions. This ensures that even if we feed the model in the same way each time, it will generate different sequences of text.\n",
        "\n",
        "To achieve this desired behavior, we can employ a technique known as random sampling. When presented with an array of log scores for the N characters in our dictionary, we add an array of random numbers to this data. The extent of randomness introduced into the predictions is regulated by a parameter called \"temperature\". By comparing the random numbers to the original input scores, the model adapts its choices, offering diversity in its outputs.\n",
        "\n",
        "This doesn't imply that the models produces entirely random results on each iteration. Rather, with each prediction, there is a probability associated with choosing a character other than the one with the highest score. This concept becomes more tangible when you explore the accompanying Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "61f601f0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "61f601f0"
      },
      "outputs": [],
      "source": [
        "def temperature_random_sampling(log_probs, temperature=1.0):\n",
        "    \"\"\"Temperature Random sampling from a categorical distribution. The higher the temperature, the more\n",
        "       random the output. If temperature is close to 0, it means that the model will just return the index\n",
        "       of the character with the highest input log_score\n",
        "\n",
        "    Args:\n",
        "        log_probs (tf.Tensor): The log scores for each characeter in the dictionary\n",
        "        temperature (number): A value to weight the random noise.\n",
        "    Returns:\n",
        "        int: The index of the selected character\n",
        "    \"\"\"\n",
        "   # Generate uniform random numbers with a slight offset to avoid log(0)\n",
        "    u = tf.random.uniform(minval=1e-6, maxval=1.0 - 1e-6, shape=log_probs.shape)\n",
        "\n",
        "    # Apply the Gumbel distribution transformation for randomness\n",
        "    g = -tf.math.log(-tf.math.log(u))\n",
        "\n",
        "    # Adjust the logits with the temperature and choose the character with the highest score\n",
        "    return tf.math.argmax(log_probs + g * temperature, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec7711f",
      "metadata": {
        "id": "eec7711f"
      },
      "source": [
        "\n",
        "Now, it's time to bring all the elements together for the exciting task of generating new text. The GenerativeModel class plays a pivotal role in this process, offering two essential functions:\n",
        "\n",
        "1. `generate_one_step`: This function is our go-to method for generating a single character at a time. It accepts two key inputs: an initial input sequence and a state that can be thought of as the ongoing context or memory of the model. The function delivers a single character prediction and an updated state, which can be used as the context for future predictions.\n",
        "\n",
        "2. `generate_n_chars`: This function takes text generation to the next level. It orchestrates the iterative generation of a sequence of characters. At each iteration, generate_one_step is called with the last generated character and the most recent state. This dynamic approach ensures that the generated text evolves organically, building upon the context and characters produced in previous steps. Each character generated in this process is collected and stored in the result list, forming the final output text.\n",
        "\n",
        "\n",
        "###  6 - GenerativeModel\n",
        "\n",
        "We will create a function to generate a single character based on the input text, using the provided vocabulary and the trained model.\n",
        "\n",
        "Follow these steps to complete the generate_one_step function:\n",
        "\n",
        "1. Start by transforming our input text into a tensor using the given vocab. This will convert the text into a format that the model can understand.\n",
        "\n",
        "2. Utilize the trained model with the input_ids and the provided states to predict the next characters. Make sure to retrieve the updated states from this prediction because they are essential for the final output.\n",
        "\n",
        "3. Since we are only interested in the next character prediction, keep only the result for the last character in the sequence.\n",
        "\n",
        "4. Employ the temperature random sampling technique to convert the vector of scores into a single character prediction. For this step, we will use the predicted_logits obtained in the previous step and the temperature parameter of the model.\n",
        "\n",
        "5. To transform the numeric prediction into a human-readable character, we will use the text_from_ids function. Be mindful that text_from_ids expects a list as its input, so we need to wrap the output of the temperature_random_sampling function in square brackets [...]. Don't forget to use self.vocab as the second parameter for character mapping.\n",
        "\n",
        "6. Finally, return the predicted_chars, which will be a single character, and the states tensor obtained from step 2. These components are essential for maintaining the sequence and generating subsequent characters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a7fde908",
      "metadata": {
        "deletable": false,
        "id": "a7fde908"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GenerativeModel(tf.keras.Model):\n",
        "    def __init__(self, model, vocab, temperature=1.0):\n",
        "        \"\"\"\n",
        "        A generative model for text generation.\n",
        "\n",
        "        Args:\n",
        "            model (tf.keras.Model): The underlying model for text generation.\n",
        "            vocab (list): A list containing the vocabulary of unique characters.\n",
        "            temperature (float, optional): A value to control the randomness of text generation. Defaults to 1.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        \"\"\"\n",
        "        Generate a single character and update the model state.\n",
        "\n",
        "        Args:\n",
        "            inputs (string): The input string to start with.\n",
        "            states (tf.Tensor): The state tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, states: The predicted character and the current GRU state.\n",
        "        \"\"\"\n",
        "        # Convert strings to token IDs.\n",
        "\n",
        "\n",
        "\n",
        "        # Transform the inputs into tensors\n",
        "        input_ids = line_to_tensor(inputs, vocab)\n",
        "        # Predict the sequence for the given input_ids. Use the states and return_state=True\n",
        "        predicted_logits, states = self.model(input_ids, states, return_state=True)\n",
        "        # Get only last element of the sequence\n",
        "        predicted_logits = predicted_logits[0, -1, :]\n",
        "        # Use the temperature_random_sampling to generate the next character.\n",
        "        predicted_ids = temperature_random_sampling(predicted_logits, self.temperature)\n",
        "        # Use the chars_from_ids to transform the code into the corresponding char\n",
        "        predicted_chars = text_from_ids([predicted_ids], vocab)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return tf.expand_dims(predicted_chars, 0), states\n",
        "\n",
        "    def generate_n_chars(self, num_chars, prefix):\n",
        "        \"\"\"\n",
        "        Generate a text sequence of a specified length, starting with a given prefix.\n",
        "\n",
        "        Args:\n",
        "            num_chars (int): The length of the output sequence.\n",
        "            prefix (string): The prefix of the sequence (also referred to as the seed).\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text sequence.\n",
        "        \"\"\"\n",
        "        states = None\n",
        "        next_char = tf.constant([prefix])\n",
        "        result = [next_char]\n",
        "        for n in range(num_chars):\n",
        "            next_char, states = self.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "\n",
        "        return tf.strings.join(result)[0].numpy().decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "47412d77",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47412d77",
        "outputId": "e52ee174-edaf-402b-ea6d-28a925bba81b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRULM--------------------------------------------------------------------------------------------------------\n",
            " me in the storm?\n",
            "DUCHESS OF YORK \n",
            "\n",
            "________________________________________________________________________________\n",
            "Dear morning, then, and say 'tis the \n",
            "\n",
            "________________________________________________________________________________\n",
            "KINGHAM\tGod pluck the market-place,  \n",
            "\n",
            "________________________________________________________________________________\n",
            "LSTMLM--------------------------------------------------------------------------------------------------------\n",
            " hear me speak to me,\n",
            "And so the  \n",
            "\n",
            "________________________________________________________________________________\n",
            "Dear Clarence,\n",
            "Who begg'd his tears, \n",
            "\n",
            "________________________________________________________________________________\n",
            "KING OF GLOUCESTER\tI think I should  \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Fix the seed to get replicable results for testing\n",
        "\n",
        "\n",
        "\n",
        "tf.random.set_seed(272)\n",
        "gen = GenerativeModel(model_gru, vocab, temperature=0.5)\n",
        "gen2 = GenerativeModel(model_lstm, vocab, temperature=0.5)\n",
        "print(\"GRULM--------------------------------------------------------------------------------------------------------\")\n",
        "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"Dear\"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"KING\"), '\\n\\n' + '_'*80)\n",
        "print(\"LSTMLM--------------------------------------------------------------------------------------------------------\")\n",
        "print(gen2.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen2.generate_n_chars(32, \"Dear\"), '\\n\\n' + '_'*80)\n",
        "print(gen2.generate_n_chars(32, \"KING\"), '\\n\\n' + '_'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c21ca85",
      "metadata": {
        "id": "7c21ca85"
      },
      "source": [
        "Now, generate a longer text. Let's check if it looks like Shakespeare fragment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "cc362c27",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc362c27",
        "outputId": "9d744c30-c3d5-44fd-b2ce-cabc14f79fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRULM--------------------------------------------------------------------------------------------------------\n",
            "ROMEO AND, Keepest of Ephesus]\n",
            "GRATIANO\tIf she did show your own clowns and thee,\n",
            "That Publius Pisoled chamber with me and in\n",
            "The officer of the news is Clarence Show.\n",
            "But I forgive thee in my rest of love.\n",
            "You mark her? say to Warwick's officers?\n",
            "Second Keeper\t[Aside]  Then if the while you may not weigh\n",
            "And please you that way saying like the\n",
            "cow to be whirl thou pitchesy and love and\n",
            "live with the tabbors to the wages:\n",
            "there is a witty wit to love: what's the news?\n",
            "EDGAR\tWhy, what rage?\n",
            "POMPEY\tYou will be with you.\n",
            "TITUS ANDRONICUS\tAy, but my father's estate to see thy sakeness:\n",
            "Thou wouldst advance their daughters for my rest,\n",
            "And let the emperor I will for thine,\n",
            "Whose name is a kind of man; for I to her\n",
            "Unto the murken to a million a little\n",
            "herald; they are not dead? who can renown the sea?\n",
            "Gentleman\tAy, Desdemona.\n",
            "GOWER\tIt is the time to live.\n",
            "HORTENSIO\tI will far him to eat over: but come!\n",
            "Tell me, as we do not to the like.\n",
            "Messenger\tThus much words follow thee what store; but I\n",
            "thin \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.371234178543091\n",
            "LSTMLM--------------------------------------------------------------------------------------------------------\n",
            "ROMEO AND GLOUCESTER\tMost rich and slave, I cannot throw at your cloak.\n",
            "QUEEN ELIZABETH\tI did approve them of my sorrow: he hath sought\n",
            "To thee, my lord; and say Shallow is not so.\n",
            "ALEXANDER\tI do not say so sincered.\n",
            "CASSIO\tMethinks, and therefore will become him there.\n",
            "BERTRAM\tWhy, then you may not take a king: she is not\n",
            "so far as well as she so great a kind of drunk.\n",
            "SILVIA\tPlease is your honour.\n",
            "HELICANUS\tAll the base, thou art a foamed fret and sweet state.\n",
            "YORK\tHonest, fool, kill, willowey you! yes, indeed,\n",
            "Will you beseech you, sir.\n",
            "Doctor\tHe that estate the moon.\n",
            "CORIOLANUS\tThou wouldst not think, Bullcandy. A fool\n",
            "To love your lordship. Fare you well.\n",
            "QUEEN MARGARET\tFetch him the trumpet. Come, sir.\n",
            "QUEEN MARGARET\tThe terrible prince cannot come to you.\n",
            "CORIOLANUS\tIs that the world?\n",
            "KING LEAR\tIs it your cur?\n",
            "LAUNCE\tWhy, Virging to him, and the contrary cries off his\n",
            "music and purpose.\n",
            "MARINA\tWhy, then, my lord.\n",
            "MARK ANTONY\tMessenger, I can for the king when your minds\n",
            "Too near the d \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.090849876403809\n"
          ]
        }
      ],
      "source": [
        "# PS : we will  only generate 1000 chars\n",
        "tf.random.set_seed(np.random.randint(1, 1000))\n",
        "print(\"GRULM--------------------------------------------------------------------------------------------------------\")\n",
        "gen = GenerativeModel(model_gru, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"ROMEO \"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)\n",
        "print(\"LSTMLM--------------------------------------------------------------------------------------------------------\")\n",
        "gen = GenerativeModel(model_lstm, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"ROMEO \"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4338c40f",
      "metadata": {
        "id": "4338c40f"
      },
      "source": [
        "In the generated text above, you can see that the models generates text that makes sense capturing dependencies between words and without any input. A simple n-gram model would have not been able to capture all of that in one sentence."
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}