# Deep-N-gram-GRU-LSTM-Language-models

![alt text](https://github.com/BheZelmat/Deep-N-gram-GRU-LSTM-Language-models/blob/main/img.png?raw=true)

## Overview
This Notebook outlines the creation of advanced language models using GRU and LSTM networks to predict the next character in a sequence, aiding in understanding the practical application of these models in natural language processing.

## Key Features

* Step-by-step guide to data preprocessing, model definition, training, and evaluation.
* Detailed comparison of GRU and LSTM network performances.
* Innovative text generation techniques employing temperature-based sampling for varied outputs.

##Dependencies

TensorFlow
NumPy

## Dataset
Explains the use of a character-based dataset for training and evaluation, focusing on sequence generation and prediction accuracy.

## Usage
Instructions for replicating the models, training them with your dataset, and using them for text generation tasks are provided in detail within the document.

## Author
Houssem Zelmat 
